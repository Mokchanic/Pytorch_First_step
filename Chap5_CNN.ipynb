{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chap5_CNN\n",
    "==========\n",
    "간단한 합성곱 신경망 구현\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn                        # 신경망\n",
    "import torch.optim as optim                  # 최적화\n",
    "import torch.nn.init as init                 # 텐서에 초기값을 주기위한 험수\n",
    "import torchvision.datasets as dset          # 모듈 데이터를 읽어옴\n",
    "import torchvision.transforms as transforms  # 불러온 이미지를 변환\n",
    "from torch.utils.data import DataLoader      # DataLoader모듈\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 256\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋을 사용.\n",
    "### dset.MNIST (28 by 28의 이미지)\n",
    "dset.MNIST(데이터경로, train=True(학습 데이터 불러옴), train=False(테스트 데이터 불러옴),  \n",
    "transform & target_transform(각각 데이터와 라벨에 대한 변형을 의미.)  \n",
    "transform.ToTensor()는 이미지 데이터를 파이토치 텐서로 변환하겠다는 의미.  \n",
    "download(현재경로에 MNIST데이터가 없을 경우에 다운로드를 하겠다는 의미.)\n",
    "* * *\n",
    "### torch.utils.data.DataLoader\n",
    "batch_size: dset.MNIST를 통해 정리된 데이터를 batch_size 개수만큼 묶는다는 의미  \n",
    "shuffle: 셔플여부(데이터를 데이터로더에서 섞어서 사용을 하겠는지 여부)  \n",
    "num_workers: 데이터를 묶을 때 사용할 프로세스 개수  \n",
    "drop_last: 묶고 남은 데이터를 버릴지에 대한 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST(\"./\", train=True, transform = transforms.ToTensor(),\n",
    "                        target_transform=None, download=True)\n",
    "mnist_test  = dset.MNIST(\"./\", train=False, transform = transforms.ToTensor(),\n",
    "                        target_transform = None, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size,\n",
    "                                          shuffle = True, num_workers = 2, drop_last = True)\n",
    "test_loader  = torch.utils.data.DataLoader(mnist_test,  batch_size = batch_size,\n",
    "                                          shuffle = False,num_workers = 2, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net CNN\n",
    "* * *\n",
    "Conv2d의 인수 (입력채널, 출력채널, kernel_size, stride, padding 등)  \n",
    "super는 CNN클래스의 부모 클래스인 nn.Module을 초기화 하는 역할을 해줌.  \n",
    "입력채널에는 RGB 3채널 데이터를 입력으로 받음.  \n",
    "16은 필터의 갯수  \n",
    "kernel_size: 5 (임의의 값)\n",
    "***\n",
    "Tensor의 형태\n",
    "--------------------\n",
    "입력: [batch_size, input_channel,  가로, 세로]  \n",
    "출력: [batch_size, output_channel, 가로, 세로]  \n",
    "\n",
    "### 연산식\n",
    "$$\n",
    "O=floor(\\frac{I-K+2P}{S}+1)\n",
    "$$\n",
    "I: 이미지의 크기(입력의 한변의 크기)  \n",
    "K: 필터(커널)의 크기  \n",
    "P: 패딩의 크기  \n",
    "S: 스트라이드 기본값 1  \n",
    "  \n",
    "문제: nn.Conv2d(1, 16, 5)를 통과한 이미지의 사이즈?\n",
    "답: 24 \n",
    "\n",
    "### Max Pooling\n",
    "nn.MaxPool2d 인수: kernel_size(풀링 연산의 크기), stride=2 풀링을 하고 2만큼 이동  \n",
    "해당 연산이후 텐서가 반으로 줄어듬.  \n",
    "결과: [batch_size, 32,  20, 20] -> [batch_size, 32,  10, 10]\n",
    "\n",
    "### torch.view  \n",
    "예를들어 [4,16]의 형태의 텐서를 torch.view(2, -1)를 거치면, [2,32]의 텐서를 얻음.\n",
    "\n",
    "### fc_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make class CNN\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),# [batch_size, 32,  10, 10]\n",
    "            nn.Conv2d(32,64,5),\n",
    "            nn.ReLU(),# [batch_size, 64,  6, 6]\n",
    "            nn.MaxPool2d(2,2) # [batch_size, 64,  3, 3]\n",
    "        )    \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*3*3,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        out = self.layer(x_input)\n",
    "        out = out.view(batch_size, -1) # -1은 -1부분을 알아서 계산하라는 의미\n",
    "        out = self.fc_layer(out) # [batch_size, 10]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN().to(device)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c17c41e6bb64ef9bb331f5efeba712b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    for j,[image, label] in enumerate(train_loader):\n",
    "        x  = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_f(output,y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 1000 == 0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        x  = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        _, output_index = torch.max(output,1)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y_).sum().float()\n",
    "        \n",
    "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
